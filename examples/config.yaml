# Example alaala configuration file
# Copy this to ~/.alaala/config.yaml and customize

storage:
  weaviate_url: http://localhost:8080  # Docker Weaviate URL (required)
  sqlite_path: ~/.alaala/alaala.db

ai:
  provider: anthropic  # "anthropic", "openrouter", or "ollama"
  api_key: ${ANTHROPIC_API_KEY}  # or ${OPENROUTER_API_KEY} (not needed for ollama)
  model: claude-3-5-sonnet-20241022  # Model name (provider-specific)
  openrouter_url: https://openrouter.ai/api/v1  # Optional
  ollama_url: http://localhost:11434  # Optional (default)

embeddings:
  provider: local  # "local" (placeholder) or "ollama" (real embeddings)
  model: all-MiniLM-L6-v2  # or "nomic-embed-text" for ollama
  ollama_url: http://localhost:11434  # Optional (default)

retrieval:
  max_memories: 5  # Maximum memories to return
  min_importance: 0.3  # Minimum importance threshold (0-1)
  include_graph_depth: 1  # Follow memory relationships (0 = disabled)

logging:
  level: info  # "debug", "info", "warn", "error"
  file: ~/.alaala/alaala.log

# Example: Local AI with Ollama (fully private, no API costs)
# ai:
#   provider: ollama
#   model: llama3.1
#   ollama_url: http://localhost:11434
# embeddings:
#   provider: ollama
#   model: nomic-embed-text
#   ollama_url: http://localhost:11434
#
# Setup: brew install ollama
#        ollama pull llama3.1
#        ollama pull nomic-embed-text

# Example OpenRouter Configuration (Multiple Models):
# ai:
#   provider: openrouter
#   api_key: ${OPENROUTER_API_KEY}
#   model: anthropic/claude-3.5-sonnet  # Best quality
#   # Other options:
#   # model: openai/gpt-4-turbo         # Fast, reliable
#   # model: meta-llama/llama-3.1-70b-instruct  # Cost-effective
#   # model: google/gemini-pro-1.5      # Fast, cheap
#   # model: meta-llama/llama-3.1-8b-instruct:free  # Free tier!

# Popular OpenRouter Models for Memory Curation:
# - meta-llama/llama-3.1-8b-instruct:free - Best free option
# - anthropic/claude-3.5-sonnet - Best quality, ~$3/$15 per 1M tokens
# - openai/gpt-4-turbo - Fast & reliable, ~$10/$30 per 1M tokens
# - meta-llama/llama-3.1-70b-instruct - Cost-effective, ~$0.50/$0.80 per 1M tokens
# - google/gemini-flash-1.5:free - Fast & free
