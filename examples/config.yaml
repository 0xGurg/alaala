# Example alaala configuration file
# Copy this to ~/.alaala/config.yaml and customize

storage:
  weaviate_url: http://localhost:8080  # Docker Weaviate URL (required)
  sqlite_path: ~/.alaala/alaala.db

ai:
  provider: anthropic  # "anthropic" or "openrouter"
  api_key: ${ANTHROPIC_API_KEY}  # Use environment variable or set directly
  model: claude-3-5-sonnet-20241022  # Model name (provider-specific)
  openrouter_url: https://openrouter.ai/api/v1  # Only needed if using openrouter (optional)

embeddings:
  provider: local  # Only "local" is currently supported (placeholder embeddings)
  model: all-MiniLM-L6-v2

retrieval:
  max_memories: 5  # Maximum memories to return in search
  min_importance: 0.3  # Minimum importance threshold (0-1)

logging:
  level: info  # "debug", "info", "warn", "error"
  file: ~/.alaala/alaala.log

# Example OpenRouter Configuration (Multiple Models):
# ai:
#   provider: openrouter
#   api_key: ${OPENROUTER_API_KEY}
#   model: anthropic/claude-3.5-sonnet  # Best quality
#   # Other options:
#   # model: openai/gpt-4-turbo         # Fast, reliable
#   # model: meta-llama/llama-3.1-70b-instruct  # Cost-effective
#   # model: google/gemini-pro-1.5      # Fast, cheap
#   # model: meta-llama/llama-3.1-8b-instruct:free  # Free tier!

# Popular OpenRouter Models for Memory Curation:
# - meta-llama/llama-3.1-8b-instruct:free - Best free option
# - anthropic/claude-3.5-sonnet - Best quality, ~$3/$15 per 1M tokens
# - openai/gpt-4-turbo - Fast & reliable, ~$10/$30 per 1M tokens
# - meta-llama/llama-3.1-70b-instruct - Cost-effective, ~$0.50/$0.80 per 1M tokens
# - google/gemini-flash-1.5:free - Fast & free
